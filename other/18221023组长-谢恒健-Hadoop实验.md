# Hadoop

## Hadoop是什么

* Hadoop是一个由Apache基金会所开发的分布式系统基础架构
* 主要解决海量数据的存储和分析计算问题
* 广义上来说，Hadoop通常是指一个更广泛的概念——Hadoop生态圈

## 发展历史

* Lucene是Doug cutting开创的开源软件，用java书写代码，实现与谷歌类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询引擎和索引引擎
* 2001年年底Lucene成为Apache基金会的一个子项目
* 对于海量数据的场景，Lucene面对与谷歌同样的困难，存储数据困难，检索速度慢
* 学习和模仿谷歌解决这些问题的办法：微信Nutch
* 谷歌是Hadoop的思想之源（谷歌在大数据方面的三篇论文）
  * GFS——>HDFS
  * Map-Reduce——>MR
  * BigTable——>HBase
* 2003-2004年，谷歌公开了部分GFS和MapReduce思想的细节，依次为基础Doug cutting等人用2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升
* 2005年Hadoop作为Lucene的子项目Nutch的一部分正式引入Apache基金会
* 2006年3月份，Map-Reduce的Nutch Distributed file system分别纳入称为Hadoop的项目中
* Hadoop就此诞生且快速发展，标志着大数据时代来临

## Hadoop三大发行版本

### Apache

最原始的版本，入门学习最好

### Cloudera

在大型互联网企业中用的较多

### Hortonworks

文档较好

## Hadoop的优势

* 高可靠性：Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失
* 高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点
* 高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度
* 高容错性：能够自动将失败的任务重新分配

## Hadoop组成

### Hadoop 1.x组成

* MapReduce:计算+调度资源
* HDFS：数据存储
* Common：辅助工具

### Hadoop 2.x组成

* MapReduce:计算
* Yarn：资源调度
* HDFS：数据存储
* Common：辅助工具

### HDFS架构概述

* NameNode(nn):存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等
* DataNode(dn):在本地文件系统存储文件块数据，以及块数据的校验和
* Secondary NameNode(2nn):用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照

### Yarn架构概述

#### ResourceManager(RM)

* 处理客户端请求
* 监控NodeManager
* 启动或监控ApplicationMaster

#### NodeManager(NM)

* 管理单个节点上的资源
* 处理来自ResourceManager的命令
* 处理来自ApplicationMaster的命令

#### ApplicationMaster(AM)

* 负责数据的切分
* 为应用程序申请资源并分配给内部的任务
* 任务的监控与容错

#### Container

Container是YARN中的资源抽象，它封装了某个节点上的多个维度资源，如内存、cpu、磁盘，网络等

### MapReduce架构概述

> MapReduce将计算过程分为两个阶段：Map和Reduce

* Map阶段并行处理输入数据
* Reduce阶段对Map结果进行汇总

## 大数据生态体系

### 数据来源层

* 数据库（结构化数据）
* 文件日志（半结构化数据）
* 视频、ppt等（非结构化数据）

### 数据传输层

* Sqoop数据传递
* Flume日志收集
* kafka消息队列

### 数据存储层

* HBase非关系型数据库
* HDFS文件存储
* Kffka消息队列

### 资源管理层

* YARN资源管理

### 数据计算层

* MapReduce离线计算
  * Hive数据查询
  * Mahout数据挖掘
* Spark Core内存计算
  * Maout数据挖掘
  * Spark Mlib数据挖掘
  * Spark R数据分析
  * Spark Sql数据查询
  * Spark streaming实时计算
* storm实时计算

### 任务调度层

* Oozie任务调度
* Azkaban任务调度

### 业务模型层

* 业务模型
* 数据可视化
* 业务应用

## 环境搭建

> 实验中可能会涉及如curl、wget、vim等基础工具，本文略去安装过程以节省篇幅。

### 虚拟机准备

#### 安装虚拟机

>  本次实验我使用了MAC平台的Parallels虚拟机平台来建立虚拟机。镜像源为阿里云的Ubuntu18.04

建立三台虚拟机，分配为`Hadoop-1`、`Hadoop-2`、`Hadoop-3`

成功启动后，按下图所示安装`Parallels Tools`可实现主机与虚拟机的直接通信

![image-20201101185937601](http://img.hjxie.icu/image-20201101185937601.png)

![image-20201101190108055](http://img.hjxie.icu/image-20201101190108055.png)

#### 关闭防火墙

> 使用Parallels安装最小版的ubuntu没有防火墙，故不需要关闭

#### 创建用户

> 新建 hadoop 用户，这个用户专门用来维护集群，因为实际中使用 root 用户的机会很少，而且不安全。

```shell
# groupadd hadoop
# useradd -s /bin/bash -g hadoop -d /home/hadoop -m hadoop
# passwd hadoop
```



![image-20201101195200113](http://img.hjxie.icu/image-20201101195200113.png)

#### 准备SSH环境

> 使用Parallels安装的虚拟机不需要配置网卡，但是需要安装ssh的server端才可以接收ssh请求

```shell
$ sudo apt install openssh-server
```

#### 修改HOSTS

查看三台虚拟机的ip地址

![image-20201101203112170](http://img.hjxie.icu/image-20201101203112170.png)

![image-20201101203152453](http://img.hjxie.icu/image-20201101203152453.png)

![image-20201101203239156](http://img.hjxie.icu/image-20201101203239156.png)

将如下内容添加到`/etc/hosts`中

```txt
10.211.55.33 cluster1
10.211.55.34 cluster2
10.211.55.35 cluster3
```

测试

![image-20201101203441418](http://img.hjxie.icu/image-20201101203441418.png)

#### 秘钥配置

> 只需在cluster1上执行

##### 生成秘钥

```shell
$ ssh-keygen -t rsa
```

![image-20201101203730740](http://img.hjxie.icu/image-20201101203730740.png)

##### 分发秘钥

```shell
$ ssh-copy-id cluster1
$ ssh-copy-id cluster2
$ ssh-copy-id cluster3
```

![image-20201101204838706](http://img.hjxie.icu/image-20201101204838706.png)

![image-20201101204905433](http://img.hjxie.icu/image-20201101204905433.png)

![image-20201101204925641](http://img.hjxie.icu/image-20201101204925641.png)

### 软件安装

#### 基础依赖安装

> 以下命令三台虚拟机均执行

```shell
$ sudo apt install perl*
$ sudo apt install ntpdate
$ sudo apt install libaio-dev
$ sudo apt install screen
$ sudo apt install ntp
$ sudo apt install openjdk-8-jdk
```

修改ntp配置

在cluster1中找到/etc/ntp.conf文件，找到以下内容并注释

```conf
pool 0.ubuntu.pool.ntp.org iburst
pool 1.ubuntu.pool.ntp.org iburst
pool 2.ubuntu.pool.ntp.org iburst
pool 3.ubuntu.pool.ntp.org iburst
```

在最下面加入以下内容

```conf
restrict default ignore
restrict 10.0.2.0 mask 255.255.255.0 nomodify notrap 
server 127.127.1.0
```

重启ntp

```shell
$ sudo service ntp restart
```



在cluster2和cluster3中设定定时任务来向服务器同步日志

```shell
$crontab -e
```

输入如下内容后保存退出

```shell
0 0 * * * /usr/sbin/ntpdate cluster1>> /root/ntpd.log
```

#### MySQL安装

> 在cluster2上安装mysql

##### 安装服务端

```shell
$ sudo apt-get install mysql-server
```

##### 安装客户端

```shell
$ sudo apt-get install mysql-client
```

##### 新建Mysql用户

> mysql安装后会自动创建mysql用户与用户组，不需要专门创建

登录mysql测试

![image-20201101224233018](http://img.hjxie.icu/image-20201101224233018.png)

配置访问权限

![image-20201101224532395](http://img.hjxie.icu/image-20201101224532395.png)

#### 安装Zookeeper

> 三台均安装

```shell
$ sudo apt install Zookeeper
```

找到/etc/zookeeper/conf/zoo.cfg文件，进行如下修改,并创建相关文件夹

```conf
dataDir=/home/hadoop_files/hadoop_data/zookeeper
dataLogDir=/home/hadoop_files/hadoop_logs/zookeeper/dataLog
server.1=cluster1:2888:3888
server.2=cluster2:2888:3888
server.3=cluster3:2888:3888
```

 data 目录中创建一个文件 myid，输入内容为 编号, myid 应与 zoo.cfg 中的集群节点相匹配

```shell
# echo "1" >> /home/hadoop_files/hadoop_data/zookeeper/myid
//1号机输入1，2号机器输入2，3号机器输入3
```

找到`/usr/share/zookeeper/bin/zkEnv.sh`，按如下修改

```sh
if [ "x${ZOO_LOG_DIR}" = "x" ]
then
	ZOO_LOG_DIR="/home/hadoop_files/hadoop_logs/zookeeper/logs" fi
if [ "x${ZOO_LOG4J_PROP}" = "x" ] 
then
	ZOO_LOG4J_PROP="INFO,ROLLINGFILE" 
fi
```

找到`/etc/zookeeper/conf/log4j.properties`，按如下修改

```txt
zookeeper.root.logger=INFO,ROLLINGFILE log4j.appender.ROLLINGFILE=org.apache.log4j.DailyRollingFileAppender
```



#### 启动zookeeper

> 三台依次执行

```shell
$ cd  /usr/share/zookeeper/bin
$ ./zkServer.sh start
```

JPS查看进程

![image-20201101230025743](http://img.hjxie.icu/image-20201101230025743.png)

#### 安装Kafka

```shell
$ cd /usr/local
$ wget http://mirror.bit.edu.cn/apache/kafka/2.3.1/kafka_2.11-2.3.1.tgz
$ tar -zxvf kafka_2.11-2.3.1.tgz
$ mv kafka_2.11-2.3.1 kafka
```

设置环境变量

将如下内容添加到/etc/profile中

```profile
export KAFKA_HOME=/usr/local/kafka
export PATH=$PATH:$KAFKA_HOME/bin:$PATH
```

刷新

```shell
$ source /etc/profile
```

找到/usr/local/kafka/config/server.properties文件，按如下修改

```txt
broker.id=1 //或2、3
log.dirs=/home/hadoop_files/hadoop_logs/kafka
zookeeper.connect=cluster1:2181,cluster2:2181,cluster3:2181
advertised.host.name=10.211.55.33
```

创建集群

```shell
$ kafka-server-start.sh /usr/local/kafka/config/server.properties &
```



创建topic

```shell
$ kafka-topics.sh --create --zookeeper cluster1:2181,cluster2:2181,cluster3:2181 --replication-factor 3 --partitions 1 --topic mykafka
```

![image-20201101233537798](http://img.hjxie.icu/image-20201101233537798.png)

查看topic

```shell
$ kafka-topics.sh --list --zookeeper cluster1:2181,cluster2:2181,cluster3:2181

```

![image-20201101233642702](http://img.hjxie.icu/image-20201101233642702.png)

#### 安装Hadoop

> Hadoop 启动的先决条件是 zookeeper 已经成功启动

```shell
$ cd /usr/local
$ wget http://mirrors.shu.edu.cn/apache/hadoop/common/hadoop-3.1.2/hadoop-3.1.2.tar.gz
$ tar -zxvf hadoop-3.1.2.tar.gz
$ mv hadoop-3.1.2 hadoop
```

找到`/usr/local/hadoop/etc/hadoop/hadoop-env.sh`文件，按如下修改

```sh
export HADOOP_PID_DIR=/home/hadoop_files
//使用openjdk安装，java目录不需要修改
```

找到`/usr/local/hadoop/etc/hadoop/mapred-env.sh`文件，按如下修改

```sh
export HADOOP_MAPRED_PID_DIR=/home/hadoop_files
```

安装指导书修改相关的xml文件，由于文件占用篇幅过大，此处不做展示。

修改环境变量

```txt
export HADOOP_HOME=/usr/local/hadoop-2.6.5
export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
```

分别在三台设备上启动zookeeper、journalnode、

```shell
$ zkServer.sh start
$ hadoop-daemon.sh start journalnode
```

在cluster1上启动HDFS与YARN

```shell
$ hdfs namenode -format
$ hadoop-daemon.sh stop journalnode
$ start-dfs.sh
$ start-yarn.sh
```

启动后可在浏览器中访问`10.211.55.33:50070`查看web页面

![image-20201102001904054](http://img.hjxie.icu/image-20201102001904054.png)

#### 安装Hbase

> HBase 启动的先决条件是 zookeeper 和 Hadoop 已经启动

```shell
$ cd /usr/local
$ wget https://mirrors.tuna.tsinghua.edu.cn/apache/hbase/2.0.4/hbase-2.0.4-bin.tar.gz
$ tar -zxvf hbase-2.0.4-bin.tar.gz
$ mv hbase-2.0.4-bin hbase
```

找到`hbase-env.sh`文件,按如下修改

```sh
export HADOOP_HOME=/usr/local/hadoop
export HBASE_LOG_DIR=/home/hadoop_files/hadoop_logs/hbase/logs
export HBASE_MANAGES_ZK=false
export HBASE_PID_DIR=/home/hadoop_files
```

安装指导书修改相关的xml文件，由于文件占用篇幅过大，此处不做展示。

修改环境变量

```
export HBASE_HOME=/usr/local/hbase
export PATH=$HBASE_HOME/bin:$PATH
```

在cluster1上按之前介绍过的命令启动`zookeeper`、`HDFS`、`YARN`，然后启动`HBASE`

```shell
start-hbase.sh
```

然后访问`10.211.55.33:60010`可查看web页面

![image-20201102003305937](http://img.hjxie.icu/image-20201102003305937.png)

进入Hbaseshell

![image-20201102004102364](http://img.hjxie.icu/image-20201102004102364.png)

在浏览器也可以看到表

![image-20201102004136245](http://img.hjxie.icu/image-20201102004136245.png)

#### 安装Hive

> 以下内容除在 MySQL 中创建 hive 用户和创建 hive 数据库只用操作一次，其他操作需要在每个 Hadoop 结 点上都执行一次。

```shell
$ cd /usr/local
$ wget https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz
$ tar -zxvf apache-hive-1.2.2-bin.tar.gz 
$ mv apache-hive-1.2.2-bin hive
```

在`/etc/profile`中追加如下内容

```txt]
export HIVE_HOME=/usr/local/hive
export PATH=$HIVE_HOME/bin:$HIVE_HOME/conf:$PATH
```

登录Mysql并创建用户hive

```shell
mysql> GRANT USAGE ON *.* TO 'hive'@'%' IDENTIFIED BY 'hive' WITH GRANT OPTION;
mysql> create database hive;
mysql> grant all on hive.* to hive@'%' identified by 'hive';
mysql> grant all on hive.* to hive@'localhost' identified by 'hive';
mysql> grant all on hive.* to hive@'cluster2' identified by 'hive';
mysql> flush privileges;
mysql> exit;
```

验证hive用户

![image-20201102004557341](http://img.hjxie.icu/image-20201102004557341.png)



#### 安装Scala

> 三台设备

```shell
$ sudo apt install scala -y
```

查看版本

![image-20201102004840574](http://img.hjxie.icu/image-20201102004840574.png)

Apt安装的scala不需要配置环境变量及权限

#### 安装Spark

```shell
$ sudo apt install spark -y
```

复制 conf 文件夹里面 template 一份，改名为 spark-env.sh

```shell
$ cp conf/spark-env.sh.template conf/spark-env.sh
```

且加入如下内容

```sh
export SPARK_MASTER_IP=cluster1
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop-2.6.5/bin/hadoop classpath) export SPARK_CLASSPATH=$HIVE_HOME/lib/mysql-connector-java-5.1.43-bin.jar export SPARK_PID_DIR=/home/hadoop_files
```

在 conf 下面新建一个叫 slaves 的文，加入如下内容

```
cluster1
cluster2 
cluster3
```

将 hive 目录下 conf 文件夹中的 hive-site.xml 复制到 spark 的 conf 目录下

将 hadoop/etc/hadoop 文件中的 hdfs-site.xml 和 core-site.xml 文件复制到 spark 的 conf 目录下

spark-defaults.conf.template不需复制，apt安装的spark已经配置好了

启动Spark

```shell
$ start-master.sh
$ start-slaves.sh
```

访问`10.211.55.33:8080`查看内容

![spark-docker-ui](http://img.hjxie.icu/spark-docker-ui-600x262.png)



#### 安装Storm

> 安装storm 需要 python2.6 以上的版本，Ubuntu18.04自带python3，不存在版本问题

```shell
$ cd /usr/local
$ wget https://github.com/nathanmarz/storm/downloads/storm-0.7.1.zip
$ unzip storm-0.7.1.zip
```

在`/etc/profile`中追加如下内容

```txt
export STORM_HOME=/usr/local/storm-0.7.1 
export PATH=$STORM_HOME/bin:$PATH
```

找到`/usr/local/storm-0.7.1/conf/storm.yaml`,作如下更改

```yaml
storm.zookeeper.servers :
  - “cluster1”
  - “cluster2”
  - “cluster3”
```

并追加一行

```yaml
storm.local.dir : "/home/hadoop_files/hadoop_tmp/storm/tmp"
```

在cluster1新建虚拟窗口

```shell
$ screen -S storm-master
$ storm nimbus
$ Ctrl+A+D

$ screen -S storm-ui
$ storm ui
$ Ctrl+A+D
```

在cluster2和3中新建虚拟窗口

```shell
$ screen -S storm-supervisor
$ storm supervisor
$ Ctrl+A+D

$ screen -S storm-logviewer
$ storm logviewer
$ Ctrl+A+D
```

查看进程

![image-20201102010205812](http://img.hjxie.icu/image-20201102010205812.png)

![image-20201102010307892](http://img.hjxie.icu/image-20201102010307892.png)

## 拓展

使用虚拟机搭建hadoop集群占用了大量资源，使用docker来搭建集群会节省很多资源，在此给出一些简单的方案

安装docker

```shell
curl hjxie.icu:8000/docker/install/sh |sh
#这是我本人提供的一个快速安装docker脚本，可以直接安装带阿里云加速的docker
```

Dockerfile

```dockerfile
FROM ubuntu:18.04

MAINTAINER xiehengjian

WORKDIR /root

# install openssh-server, openjdk and wget
RUN apt-get update && apt-get install -y openssh-server openjdk-8-jdk wget

# install hadoop 2.7.2
RUN wget https://github.com/kiwenlau/compile-hadoop/releases/download/2.7.2/hadoop-2.7.2.tar.gz && \
    tar -xzvf hadoop-2.7.2.tar.gz && \
    mv hadoop-2.7.2 /usr/local/hadoop && \
    rm hadoop-2.7.2.tar.gz

# set environment variable
ENV JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64 
ENV HADOOP_HOME=/usr/local/hadoop 
ENV PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin 

# ssh without key
RUN ssh-keygen -t rsa -f ~/.ssh/id_rsa -P '' && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

RUN mkdir -p ~/hdfs/namenode && \ 
    mkdir -p ~/hdfs/datanode && \
    mkdir $HADOOP_HOME/logs

COPY config/* /tmp/

RUN mv /tmp/ssh_config ~/.ssh/config && \
    mv /tmp/hadoop-env.sh /usr/local/hadoop/etc/hadoop/hadoop-env.sh && \
    mv /tmp/hdfs-site.xml $HADOOP_HOME/etc/hadoop/hdfs-site.xml && \ 
    mv /tmp/core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml && \
    mv /tmp/mapred-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml && \
    mv /tmp/yarn-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml && \
    mv /tmp/slaves $HADOOP_HOME/etc/hadoop/slaves && \
    mv /tmp/start-hadoop.sh ~/start-hadoop.sh && \
    mv /tmp/run-wordcount.sh ~/run-wordcount.sh

RUN chmod +x ~/start-hadoop.sh && \
    chmod +x ~/run-wordcount.sh && \
    chmod +x $HADOOP_HOME/sbin/start-dfs.sh && \
    chmod +x $HADOOP_HOME/sbin/start-yarn.sh 

# format namenode
RUN /usr/local/hadoop/bin/hdfs namenode -format

CMD [ "sh", "-c", "service ssh start; bash"]
```



Start.sh

```sh
#!/bin/bash

# the default node number is 3
N=${1:-3}


# start hadoop master container
sudo docker rm -f hadoop-master &> /dev/null
echo "start hadoop-master container..."
sudo docker run -itd \
                --net=hadoop \
                -p 50070:50070 \
                -p 8088:8088 \
                --name hadoop-master \
                --hostname hadoop-master \
                kiwenlau/hadoop:1.0 &> /dev/null


# start hadoop slave container
i=1
while [ $i -lt $N ]
do
	sudo docker rm -f hadoop-slave$i &> /dev/null
	echo "start hadoop-slave$i container..."
	sudo docker run -itd \
	                --net=hadoop \
	                --name hadoop-slave$i \
	                --hostname hadoop-slave$i \
	                kiwenlau/hadoop:1.0 &> /dev/null
	i=$(( $i + 1 ))
done 

# get into hadoop master container
sudo docker exec -it hadoop-master bash
```



创建网络

```shell
$ sudo docker network create --driver=bridge hadoop
```

创建集群

```shell
$ sudo ./start.sh
```

创建后会进入容器，此时执行启动脚本，成功运行

```shell
# ./start-hadoop.sh
```

